View(df1)
rollapplydf <- function(xx, width) {
l <- length(xx)
sq <- Map(':', 1:(l-width+1), width:l)
lst <- lapply(sq, function(i) lm(xx[i] ~ seq(length(xx[i])))$coeff[2] )
do.call('rbind', c(rep(NA, width-1L), lst))
}
df2<-df1 %>%
group_by(year) %>%
mutate(beta1 = rollapplydf(xx = site, width = 5) )
View(df2)
summary(df2)
summary(df2$beta1)
class(df2$beta1)
head(df2)
df2<-df1 %>%
group_by(year) %>%
mutate(beta1 = rollapplydf(xx = site, width = 5) ) %>%
mutate(beta1 = as.vector(beta1))
View(df2)
View(df2)
rollapplydf <- function(xx, width) {
l <- length(xx)
sq <- Map(':', 1:(l-width+1), width:l)
lst <- lapply(sq, function(i) lm(xx[i] ~ seq(length(xx[i])))$coef[2] )
do.call('rbind', c(rep(NA, width-1L), lst))
}
df2<-df1 %>%
group_by(year) %>%
mutate(beta1 = rollapplydf(xx = site, width = 5) ) %>%
mutate(beta1 = as.vector(beta1))
View(df2)
## lms is a function which calculate the linear regression coefficient
lms <- function(y, x){
s = which(is.finite(x * y))
y = y[s]
x = x[s]
return(cov(x, y)/var(x))
}
z <- data.frame()
## x has to be ungrouped
x <- ungroup(x)
library(dplyr)
n.dates <- 60
n.stocks <- 2
date <- seq(as.Date("2011-07-01"), by=1, len=n.dates)
symbol <- replicate(n.stocks, paste0(sample(LETTERS, 5), collapse = ""))
x <- expand.grid(date, symbol)
x$return <- rnorm(n.dates*n.stocks, 0, sd = 0.05)
names(x) <- c("date", "company", "return")
With this dataframe, I can calculate the daily market average return and add that result into a new column "market.ret".
x <- group_by(x, date)
x <- mutate(x, market.ret = mean(x$return, na.rm = TRUE))
Now I want to group all my data by different companies (2 in this case).
x <- group_by(x, company)
#################################
lms <- function(y, x){
s = which(is.finite(x * y))
y = y[s]
x = x[s]
return(cov(x, y)/var(x))
}
## z is a dataframe which stores our final result
z <- data.frame()
## x has to be ungrouped
x <- ungroup(x)
symbols <- unique(x$company)
View(x)
symbols <- unique(x$company)
for(i in 1:length(symbols)){
temp <- filter(x, company == symbols[i])
z <- rbind(z, mutate(temp, beta = rollapply(temp[, c(3, 4)],
FUN = function(x) lms(x[, 1], x[, 2]),
width = 20, fill = NA,
by.column = FALSE, align = "right")))
}
print(z)
View(z)
library(data.table)
run.rolling.regressions <- function(x) {
DT <- data.table(   Y = rnorm(10000),
X = rnorm(10000),
key.group = rep(LETTERS[1:10], each = 1000))
window.length <- 12
names.of.groups <- unique(DT$key.group)
number.of.groups <- length(names.of.groups)
X.coefficients <- list()
for(j in 1:length(names.of.groups)) {
regressed.DT <- DT[key.group == names.of.groups[j]]
nrows.of.group <- nrow(regressed.DT)
print(paste0(j, ', key.group: ', names.of.groups[j]))
for (i in window.length:nrows.of.group) {
if(i == window.length) {
X.coefficients[[names.of.groups[j]]] <- c(rep(NA, nrows.of.group)) }
X.coefficients[[names.of.groups[j]]][i] <-  lm(Y ~ 1 + X,
data = regressed.DT[(i - 11):i])$coefficients['X']
}
}
return(X.coefficients)
}
system.time(X.coef <- run.rolling.regressions())
unlist(X.coef)
library(RcppRoll)
rolling2 <- function(DT, window.length) {
setNames(lapply(unique(DT$key.group), function(g) {
regressed.DT <- DT[key.group == g]
xyBar = roll_mean(regressed.DT$X*regressed.DT$Y, window.length)
xBar = roll_mean(regressed.DT$X, window.length)
yBar = roll_mean(regressed.DT$Y, window.length)
x2Bar = roll_mean(regressed.DT$X^2, window.length)
c(rep(NA, window.length-1), (xyBar - xBar*yBar) / (x2Bar - xBar^2))
}), unique(DT$key.group))
}
install.packages("RcppRoll")
library(RcppRoll)
rolling2 <- function(DT, window.length) {
setNames(lapply(unique(DT$key.group), function(g) {
regressed.DT <- DT[key.group == g]
xyBar = roll_mean(regressed.DT$X*regressed.DT$Y, window.length)
xBar = roll_mean(regressed.DT$X, window.length)
yBar = roll_mean(regressed.DT$Y, window.length)
x2Bar = roll_mean(regressed.DT$X^2, window.length)
c(rep(NA, window.length-1), (xyBar - xBar*yBar) / (x2Bar - xBar^2))
}), unique(DT$key.group))
}
set.seed(144)
DT <- data.table(   Y = rnorm(10000),
X = rnorm(10000),
key.group = rep(LETTERS[1:10], each = 1000))
View(DT)
? run.rolling.regressions
system.time(X.coef <- run.rolling.regressions(DT, 12))
system.time(X.coef2 <- rolling2(DT, 12))
X.coef2
library(RcppRoll)
rolling2 <- function(DT, window.length) {
setNames(lapply(unique(DT$key.group), function(g) {
regressed.DT <- DT[key.group == g]
xyBar = roll_mean(regressed.DT$X*regressed.DT$Y, window.length)
xBar = roll_mean(regressed.DT$X, window.length)
yBar = roll_mean(regressed.DT$Y, window.length)
x2Bar = roll_mean(regressed.DT$X^2, window.length)
c(rep(NA, window.length-1), (xyBar - xBar*yBar) / (x2Bar - xBar^2))
}), unique(DT$key.group))
}
summary(DT$key.group)
View(DT)
(3068.7-2888.23)/2888.23
((3068.7-2888.23)/2888.23)*100
3068.7-2888.23
3068.7-2888.23
? rep
library(ggplot2)
? facet_wrap
ggplot(mpg, aes(displ, hwy)) +
geom_point() +
facet_wrap(~ cyl + drv)
ggplot(mpg, aes(displ, hwy)) +
geom_point() +
facet_wrap(~ cyl)
ggplot(mpg, aes(displ, hwy)) +
geom_point() +
facet_wrap(~ cyl + drv)
ggplot(mpg, aes(displ, hwy)) +
geom_point() +
facet_wrap(~ cyl)
ggplot(mpg, aes(displ, hwy)) +
geom_point() +
facet_wrap(~ cyl + drv)
ggplot(mpg, aes(displ, hwy)) +
geom_point() +
facet_wrap(~ drv+ cyl)
4*64
(4*64)/60
50*10
50*8
50*8
47.16*8
47.16*8
install.packages("readxl")
? opts_chunk
class(C(aasd,dfgts,asa))
list.of.packages <- c("ggplot2", "Rcpp")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
list.of.packages <- c("ggplot2", "Rcpp", "genetics")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
new.packages
if(length(new.packages)) install.packages(new.packages)
list.of.packages
lapply(x, list.of.packages, character.only = TRUE)
list.of.packages <- c("ggplot2", "dplyr", "tidyr", "stringr", "lubridate", "readxl")
lapply(list.of.packages, require, character.only = TRUE)
Sub507_Analysed <- read.table("//ad.monash.edu/home/User088/newmand/Desktop/Sub507_Analysed.csv", quote="\"", comment.char="")
View(Sub507_Analysed)
source('~/.active-rstudio-document', echo=TRUE)
View(Sub507_Analysed)
library(retimes)
install.packages("retimes")
source('~/.active-rstudio-document', echo=TRUE)
library(retimes)
timefit(Sub507_Analysed)
timefit(Sub507_Analysed$V1)
####Which computer/directory is this being run on?
location<-"Monash"
# location<-"DansLaptop"
if (location=="Monash") {
setwd(("C:/GitHub/BlueEnrichedLightRepo/Analyses Scripts_R"))
} else if (location=="DansLaptop") {
setwd(("C:/Users/Dan/Documents/GitHub/BlueEnrichedLightRepo/Analyses Scripts_R"))
} else setwd(("~"))
####################################
#######  How to use ################
####################################
# 1) Install the packages and software specified below. (consider also updating all installed packages by chosing Update on the Packages tab)
# 2) Set the working directory (setwd) above, and directory where you have "data_ParticipantLevel", "master_matrix_R" and "ID_vector" saved
# 3) Hit Knit Word (or Knit HTML)!
####################################
######  FIRST TIME ONLY ############
####################################
#Remove # in front of the line below and run the code. Replace the # after installing the packages, otherwise the R markdown script will give errors.
# install.packages(c("MASS", "akima", "robustbase", "cobs", "robust", "mgcv", "scatterplot3d", "quantreg", "rrcov", "lars", "pwr", "trimcluster", "mc2d", "psych", "Rfit","MBESS", "BayesFactor", "PoweR", "ggplot2", "reshape2", "plyr", "devtools", "rmarkdown","gmodels", "HLMdiag", "car", "gridExtra", "bootES", "BEST","foreign","nlme","pastecs","multcomp","ggplot2","compute.es","ez","lattice","lme4","effects","diagram","png", "grid","lmerTest", "LMERConvenienceFunctions"))
#Installation of the robust statistics package: Remove # in front of each of 4 lines below and run the code. Replace the # after installing the packages, otherwise the R markdown script will give errors.
# install.packages("devtools")
# library("devtools")
# install_github("mrxiaohe/WRScpp")
# install_github("nicebread/WRS", subdir="pkg")
# Install RePsychLing to use the Random Slopes The principal components analysis (PCA) function rePCA()
# devtools::install_github("dmbates/RePsychLing")
#Download and install JAGS to calculate Bayesian HDI: http://sourceforge.net/projects/mcmc-jags/
###################################################################################################################################
## Install relevant libraries
library(foreign)
library(car)
library(ggplot2)
library(pastecs)
library(psych)
library(plyr)
library(multcomp)
library(reshape2)
library(compute.es)
library(ez)
library(lattice)
library(lme4)
library(png)
library(grid)
library(RePsychLing)
library(LMERConvenienceFunctions)
############  Download and Import single trial data:
# data <- read.csv("C:/Users/Dan/Documents/GitHub/BlueEnrichedLightRepo/Analyses Scripts_Matlab/master_matrix_R.csv", header=FALSE)
# data <- read.csv("C:/GitHub/BlueEnrichedLightRepo/Analyses Scripts_Matlab/master_matrix_R.csv", header=FALSE)
url <- "https://raw.githubusercontent.com/DanielPNewman/BlueEnrichedLightRepo/master/Analyses%20Scripts_Matlab/master_matrix_R.csv"
f <- file.path(getwd(), "master_matrix_R.csv")
download.file(url, f)
data<-read.csv(f, header=FALSE)
############ Download and Import IDs:
# ID <- read.table("C:/Users/Dan/Documents/GitHub/BlueEnrichedLightRepo/Analyses Scripts_Matlab/ID_vector.csv", quote="\"")
# ID <- read.table("C:/GitHub/BlueEnrichedLightRepo/Analyses Scripts_Matlab/ID_vector.csv", quote="\"")
url <- "https://raw.githubusercontent.com/DanielPNewman/BlueEnrichedLightRepo/master/Analyses%20Scripts_Matlab/ID_vector.csv"
f <- file.path(getwd(), "ID_vector.csv")
download.file(url, f)
ID<-read.csv(f, header=FALSE)
#Add ID vector to "data" dataframe
data$IDnum<-data[,1]
#Replace the participant numbers with IDs:
data[,1]<-ID[,1]
#Remove the seperate ID vector now it has been included into data dataframe
rm(ID)
#Rename data columns:
data<-rename(data, c("V1"="ID", "V2"="Light","V3"="TotalTrialNumber","V4"="Trial","V5"="ITI",
"V6"="Hemifield","V7"="VerticalVisualField","V8"="MotionDirection",
"V9"="Accuracy","V10"="FixationBreak","V11"="Artefact500msPre",
"V12"="ArtefactPost","V13"="Artefact1000msPre","V14"="RejectedTrial",
"V15"="RT","V16"="PreAlphaPower","V17"="PreAlphaPower_LeftHemi",
"V18"="PreAlphaPower_RightHemi","V19"="PreAlphaAsym","V20"="PrePupilDiameter",
"V21"="Sex","V22"="Age","V23"="NumberRepeatedTrials",
"V24"="LightCondOrder","V25"="KSS_BeforeMinusAfter","V26"="LightCondsGuessedCorrectly",
"V27"="LightCondsGuessed_Confidence","V28"="Session","V29"="Quadrant",
"V30"="PreAlphaPower_all_PO_chans"))
#Make the required columns into factors:
data$Light <- factor(data$Light)
data$ITI <- factor(data$ITI)
data$Hemifield <- factor(data$Hemifield)
data$VerticalVisualField <- factor(data$VerticalVisualField)
data$MotionDirection <- factor(data$MotionDirection)
data$Sex <- factor(data$Sex)
data$LightCondOrder <- factor(data$LightCondOrder)
data$Session <- factor(data$Session)
data$Accuracy <- factor(data$Accuracy)
data$LightCondsGuessedCorrectly <- factor(data$LightCondsGuessedCorrectly)
data$Quadrant<-factor(data$Quadrant)
#Rename factor Levels:
data$Light <- revalue(data$Light, c("1"="Low", "2"="Medium", "3"="High"))
data$ITI <- revalue(data$ITI, c("1"="1800ms", "2"="2800ms", "3"="3800ms"))
data$Hemifield <- revalue(data$Hemifield, c("1"="Left", "2"="Right"))
data$VerticalVisualField <- revalue(data$VerticalVisualField, c("1"="Upper", "2"="Lower"))
data$MotionDirection <- revalue(data$MotionDirection, c("1"="Uppward", "2"="Downward"))
data$Sex <- revalue(data$Sex, c("1"="Male", "2"="Female"))
data$LightCondOrder <- revalue(data$LightCondOrder, c("1"="HML", "2"="HLM", "3"="MHL", "4"="MLH", "5"="LHM","6"="LMH"))
data$Session <- revalue(data$Session, c("1"="One", "2"="Two", "3"="Three"))
data$Accuracy <- revalue(data$Accuracy, c("1"="Hit", "0"="Miss"))
data$LightCondsGuessedCorrectly <- revalue(data$LightCondsGuessedCorrectly, c("0"="No", "1"="Yes"))
data$Quadrant<-revalue(data$Quadrant, c("1"="Left_Upper", "2"="Right_Upper", "3"="Left_Lower", "4"="Right_Lower"))
#Re-class required vectors into Logicals:
data$FixationBreak<-as.logical(data$FixationBreak)
data$Artefact500msPre<-as.logical(data$Artefact500msPre)
data$ArtefactPost<-as.logical(data$ArtefactPost)
data$Artefact1000msPre<-as.logical(data$Artefact1000msPre)
data$RejectedTrial<-as.logical(data$RejectedTrial)
#Order any ordinal factors (may have to do this the "trail" or "validTrialNum" later too)
# data$Light<-ordered(data$Light, levels = c("Low", "Medium", "High"))  #won't make light ordered, becasue by defult R uses polynomial contrasts for ordered factors, I'd rather treat light as unordered and to "treatment" contrasts, i.e. Low vs Med, Low vs High
data$Session <- ordered(data$Session, levels = c("One", "Two", "Three"))
data$ITI <- ordered(data$ITI, levels = c("1800ms", "2800ms", "3800ms"))
############  Download and Import Participant Level data:
# data_ParticipantLevel <- read.csv("C:/Users/Dan/Documents/GitHub/BlueEnrichedLightRepo/Analyses Scripts_R/ParticipantLevelMatrix_R.csv", header=FALSE)
# data_ParticipantLevel <- read.csv("C:/GitHub/BlueEnrichedLightRepo/Analyses Scripts_R/ParticipantLevelMatrix_R.csv", header=FALSE)
url <- "https://raw.githubusercontent.com/DanielPNewman/BlueEnrichedLightRepo/master/Analyses%20Scripts_R/ParticipantLevelMatrix_R.csv"
f <- file.path(getwd(), "ParticipantLevelMatrix_R.csv")
download.file(url, f)
data_ParticipantLevel<-read.csv(f, header=FALSE)
#Rename data_ParticipantLevel columns:
data_ParticipantLevel<-rename(data_ParticipantLevel, c("V1"="ID", "V2"="Light",
"V3"="LightCondOrder","V4"="LightCondsGuessedCorrectly",
"V5"="LightCondsGuessed_Confidence", "V6"="Age",
"V7"="Sex","V8"="KSS_BeforeMinusAfter","V9"="Session"))
#Make the required columns into factors:
data_ParticipantLevel$ID <- factor(data_ParticipantLevel$ID)
data_ParticipantLevel$Light <- factor(data_ParticipantLevel$Light)
data_ParticipantLevel$Sex <- factor(data_ParticipantLevel$Sex)
data_ParticipantLevel$LightCondOrder <- factor(data_ParticipantLevel$LightCondOrder)
data_ParticipantLevel$LightCondsGuessedCorrectly <- factor(data_ParticipantLevel$LightCondsGuessedCorrectly)
data_ParticipantLevel$Session <- factor(data_ParticipantLevel$Session)
#Rename factor Levels:
data_ParticipantLevel$Light <- revalue(data_ParticipantLevel$Light, c("1"="Low", "2"="Medium", "3"="High"))
data_ParticipantLevel$Sex <- revalue(data_ParticipantLevel$Sex, c("1"="Male", "2"="Female"))
data_ParticipantLevel$LightCondOrder <- revalue(data_ParticipantLevel$LightCondOrder, c("1"="HML", "2"="HLM", "3"="MHL", "4"="MLH", "5"="LHM","6"="LMH"))
data_ParticipantLevel$Session <- revalue(data_ParticipantLevel$Session, c("1"="One", "2"="Two", "3"="Three"))
data_ParticipantLevel$LightCondsGuessedCorrectly <- revalue(data_ParticipantLevel$LightCondsGuessedCorrectly, c("0"="No", "1"="Yes"))
###############Data Cleaning For Single Trial Data######################
#Remove trials with fixation-breaks and rejected trials
data<-data[!data$FixationBreak,]
data<-data[!data$RejectedTrial,]
#Check number of Trials for each participant by running the function 'length',
#on "data$RT" for each group, broken down by ID + Light
num_trials1 <- ddply(data, c("ID", "Light"), summarise,
Trials    = length(RT))
#Create a new factor Participant x Light condition
data$IDbyLight<-interaction(data$ID, data$Light) #ID by Light factor (splits data up according to session)
#Create a variable numbering all the valid trials
data$ValidTrialNum<-(matrix(unlist(tapply(data[,1], data$IDbyLight, seq_along)), nrow=length(data[,1]), byrow=T))
data$ValidTrialNum<-c(data$ValidTrialNum) #change if from a one dimensional materix to a vector
#####Calculate Accuracy by ID and Light Condition####
data$IDbyLight<-interaction(data$ID, data$Light) #ID by Light factor (splits data up according to session)
Accuracy_checker <- ddply(data, c("ID", "Light"), summarise,
Hits  = sum(Accuracy=="Hit"),
Misses = sum(Accuracy=="Miss"))
Accuracy_checker$Total=Accuracy_checker$Hits+Accuracy_checker$Misses
Accuracy_checker$Accuracy=(Accuracy_checker$Hits/Accuracy_checker$Total)*100
#Remove trials where RT=0 (i.e. they did not respond)
data<-data[data$RT!=0,]
#Remove trials where RT longer than 1000ms (i.e. after target finished)
data<-data[data$RT<1001,]
#Remove trials where RT faster than 100ms (i.e. too fast must be false alarm)
data<-data[data$RT>200,]
############################################ Log transform:
data$log_RT<-log(data$RT) #log
###########################################################
#plot again after outlier removal:
ggplot(data, aes(RT))  + geom_histogram(aes(y=..count..), colour="black", fill="white")
hist_RT <- ggplot(data, aes(RT))  + geom_histogram(aes(y=..count..), colour="black", fill="white")
hist_RT + facet_wrap(~ ID)
#####Z-score each participant's log_RT data inside Light Condition####
data$IDbyLight<-interaction(data$ID, data$Light) #ID by Light factor (splits data up according to session)
#calculate mean and sd
m <- tapply(data$log_RT,data$IDbyLight,mean)
s <- sqrt(tapply(data$log_RT,data$IDbyLight,var))
#calculate log_RT.Z and save it inside data.frame
data$RT.Z <- (data$log_RT-m[data$IDbyLight])/s[data$IDbyLight]
#check that Z scores have mean=0 and std=1
RT.Z_checker <- ddply(data, c("ID", "Light"), summarise,
N    = length(RT.Z ),
mean = round(mean(RT.Z )),
sd   = sd(RT.Z ),
se   = sd / sqrt(N))
##Remove trials where absolute RT.Z>3 (i.e. remove outlier RTs)
data<-data[!abs(data$RT.Z)>3,]
#plot again after outlier removal:
ggplot(data, aes(RT))  + geom_histogram(aes(y=..count..), colour="black", fill="white")
hist_RT <- ggplot(data, aes(RT))  + geom_histogram(aes(y=..count..), colour="black", fill="white")
hist_RT + facet_wrap(~ ID)
#Make PostAlpha contralateral (PostAlpha_c) and -Ipsilateral (PostAlpha_i) variables:
A<-data$Hemifield=="Left"
data$PostAlpha_c[A]<-data$RightHemi_PostAlpha[A]
data$PostAlpha_c[!A]<-data$LeftHemi_PostAlpha[!A]
data$PostAlpha_i[!A]<-data$RightHemi_PostAlpha[!A]
data$PostAlpha_i[A]<-data$LeftHemi_PostAlpha[A]
rm(A)
control=lmerControl(optCtrl=list(maxfun=20000)) #increase number of interations performed by lmer() to fit the model
summary(m0 <-lmer(RT ~ 1 + Light + Hemifield + Hemifield:Light +
(Light + Hemifield + Hemifield:Light | LightCondOrder/ID) +
(1|Light) +
(1|ITI) +
(1|MotionDirection) +
(1|Quadrant) +
(1|ValidTrialNum),
data = data, REML=FALSE, na.action = na.omit))
require(LMERConvenienceFunctions)
# fit basic model, just the fixed effects and random intercepts of Participants nested inside LightCondOrder
m1 <- lmer(RT ~ 1 + Hemifield + Light + Hemifield:Light +
(1|LightCondOrder/ID),
data = data, REML=FALSE, na.action = na.omit)
plot(m1)
qqnorm(m1)
RT_Light_LeftTargets<-lmer(RT ~ 1 + Light + (1 | LightCondOrder/ID) +(1|ITI) +(1|MotionDirection) + (1|Quadrant) + (1|ValidTrialNum), data=data[data$Hemifield=="Left",], REML=FALSE, na.action = na.omit)
RT_TOT_LeftTargets<-update(RT_Light_LeftTargets, .~. + ValidTrialNum)
RT_TOTbyLight_LeftTargets<-update(RT_TOT_LeftTargets, .~. + Light:ValidTrialNum)
anova(RT_Light_LeftTargets, RT_TOT_LeftTargets, RT_TOTbyLight_LeftTargets)
RT_TOT<-update(RT_HemifieldbyLight, .~. + ValidTrialNum)
RT_HemifieldbyTOT<-update(RT_TOT, .~. + Hemifield*ValidTrialNum)
RT_LightbyTOT<-update(RT_HemifieldbyTOT, .~. + Light*ValidTrialNum)
RT_HemifieldbyLightbyTOT<-update(RT_LightbyTOT, .~. + Hemifield*Light*ValidTrialNum)
anova(RT_HemifieldbyLight,RT_TOT,RT_HemifieldbyTOT,RT_LightbyTOT,RT_HemifieldbyLightbyTOT)
#Try adding random slopes for ValidTrialNum and see if the Hemifield*TOT fixed effect interaction is still significant.
summary(RT_HemifieldbyTOT_RS<-lmer(RT ~ 1 +  Light + Hemifield + Hemifield:Light + ValidTrialNum + Hemifield:ValidTrialNum +
(1 | LightCondOrder/ID) +
(1 | ITI) +
(1 | MotionDirection) +
(1 | Quadrant) +
(0 + Hemifield | ID) +
(0 + ValidTrialNum | ID),
data = data, REML=FALSE, na.action = na.omit))
#Model failed to converge: degenerate  Hessian with 6 negative eigenvalues
#Since model with random slopes for ValidTrialNum fails to converge, we have to go with
summary(RT_HemifieldbyTOT_RS<-lmerTest::lmer(RT ~ 1 +  Light + Hemifield + Hemifield:Light + ValidTrialNum + Hemifield:ValidTrialNum +
(1 | LightCondOrder/ID) +
(1 | ITI) +
(1 | MotionDirection) +
(1 | Quadrant) +
(0 + Hemifield | ID),
data = data, REML=FALSE, na.action = na.omit))
residuals_RT_FinalModel=residuals(RT_HemifieldbyTOT_RS)
plot(residuals_RT_FinalModel)
hist(residuals_RT_FinalModel)
overdisp_fun <- function(model) {
## number of variance parameters in an n-by-n variance-covariance matrix
vpars <- function(m) {
nrow(m) * (nrow(m) + 1)/2
}
# The next two lines calculate the residual degrees of freedom
model.df <- sum(sapply(VarCorr(model), vpars)) + length(fixef(model))
rdf <- nrow(model.frame(model)) - model.df
# extracts the Pearson residuals
rp <- residuals(model, type = "pearson")
Pearson.chisq <- sum(rp^2)
prat <- Pearson.chisq/rdf
# Generates a p-value. If less than 0.05, the data are overdispersed.
pval <- pchisq(Pearson.chisq, df = rdf, lower.tail = FALSE)
c(chisq = Pearson.chisq, ratio = prat, rdf = rdf, p = pval)
}
overdisp_fun(RT_HemifieldbyTOT_RS)
colvec <- c("#ff1111","#007eff") ## second colour matches lattice default
grid.arrange(plot(RT_HemifieldbyTOT_RS,type=c("p","smooth")),
plot(RT_HemifieldbyTOT_RS,sqrt(abs(resid(.)))~fitted(.),
col=ifelse(mc2$Site=="Toolik, AK",colvec[1],colvec[2]),
type=c("p","smooth"),ylab=expression(sqrt(abs(resid)))),
## "sqrt(abs(resid(x)))"),
plot(RT_HemifieldbyTOT_RS,resid(.,type="pearson")~cYear,
type=c("p","smooth")),
qqnorm(RT_HemifieldbyTOT_RS,abline=c(0,1),
col=ifelse(mc2$Site=="Toolik, AK",colvec[1],colvec[2])))
library(gridExtra)
colvec <- c("#ff1111","#007eff") ## second colour matches lattice default
grid.arrange(plot(RT_HemifieldbyTOT_RS,type=c("p","smooth")),
plot(RT_HemifieldbyTOT_RS,sqrt(abs(resid(.)))~fitted(.),
col=ifelse(mc2$Site=="Toolik, AK",colvec[1],colvec[2]),
type=c("p","smooth"),ylab=expression(sqrt(abs(resid)))),
## "sqrt(abs(resid(x)))"),
plot(RT_HemifieldbyTOT_RS,resid(.,type="pearson")~cYear,
type=c("p","smooth")),
qqnorm(RT_HemifieldbyTOT_RS,abline=c(0,1),
col=ifelse(mc2$Site=="Toolik, AK",colvec[1],colvec[2])))
colvec <- c("#ff1111","#007eff") ## second colour matches lattice default
grid.arrange(plot(RT_HemifieldbyTOT_RS,type=c("p","smooth")),
plot(RT_HemifieldbyTOT_RS,sqrt(abs(resid(.)))~fitted(.),
col=ifelse(mc2$Site=="Toolik, AK",colvec[1],colvec[2]),
type=c("p","smooth"),ylab=expression(sqrt(abs(resid)))),
## "sqrt(abs(resid(x)))"),
plot(RT_HemifieldbyTOT_RS,resid(.,type="pearson")~cYear,
type=c("p","smooth")),
qqnorm(RT_HemifieldbyTOT_RS,abline=c(0,1))
)
qqnorm(residuals_RT_FinalModel)
qqline(residuals_RT_FinalModel)
install.packages("HLMdiag")
library(HLMdiag)
HLMresid(residuals_RT_FinalModel)
HLMresid(RT_HemifieldbyTOT_RS)
HLMresid.default (RT_HemifieldbyTOT_RS)
#Simple effect of TOT on Right Hemifield Targets
summary(glht(RT_RightHemifield_TOT<-lmer(RT ~ 1 +  ValidTrialNum +
(1 | LightCondOrder/ID) +
(1 | ITI) +
(1 | MotionDirection) +
(1 | Quadrant),
data = data[data$Hemifield=="Right",], REML=FALSE, na.action = na.omit)))
? lmer
weights=varPower(form=~ValidTrialNum*I(ValidTrialNum>0))
library(QuantPsyc)
library(QuantPsyc)
View(lm.beta)
? lm
